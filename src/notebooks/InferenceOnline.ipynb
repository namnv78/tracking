{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a88b06-3c71-4cf8-af86-7469912a12ee",
   "metadata": {
    "id": "e9a88b06-3c71-4cf8-af86-7469912a12ee"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oB6JvKfS9Kyc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1634625819956,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "oB6JvKfS9Kyc",
    "outputId": "af7cca46-d5f9-4846-a3c5-c895a929d17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minh PC\\tracking\\src\n"
     ]
    }
   ],
   "source": [
    "# Directory/to/your/repo\n",
    "%cd C:/Users/Minh PC/tracking/src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48364ab-0ef9-4934-8a14-fa1ab0a3a934",
   "metadata": {
    "executionInfo": {
     "elapsed": 30439,
     "status": "ok",
     "timestamp": 1634625854405,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "d48364ab-0ef9-4934-8a14-fa1ab0a3a934"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tracking\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\anaconda3\\envs\\tracking\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "import copy\n",
    "import cv2\n",
    "from cv2 import CAP_PROP_FRAME_COUNT\n",
    "import dask\n",
    "import glob\n",
    "from hungarian_algorithm import algorithm\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from PIL.Image import Image\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import distance_matrix\n",
    "import statistics\n",
    "import shutil\n",
    "import toolz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'yolov5/')\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages, letterbox\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "from detectObj_for_Tracking import detectFromImage\n",
    "from reid_new.vehicle_embedder import VehicleEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "j9Tlf9C_qMoo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1634625854406,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "j9Tlf9C_qMoo",
    "outputId": "e0d42dcc-80db-47e4-baa4-9f85320c1036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4df6c9-46b8-48d6-89d8-3f194eb3ad92",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634625854407,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "1e4df6c9-46b8-48d6-89d8-3f194eb3ad92"
   },
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\" dot.notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0c51a7-fdd1-493d-afb6-cc7418ca2807",
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1634626332859,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "3b0c51a7-fdd1-493d-afb6-cc7418ca2807"
   },
   "outputs": [],
   "source": [
    "vid_name = 'vungtau194'\n",
    "opt = {\n",
    "    'agnostic_nms': False, \n",
    "    'batch_size': 128,\n",
    "    'conf_thres': 0.25, \n",
    "    'device': 'cuda', \n",
    "    'euclidean_num': 3,\n",
    "    'euclidean_thresh': 0.2, \n",
    "    'img_size': 640, \n",
    "    'iou_thres': 0.45, \n",
    "    # 'n_image': 100, \n",
    "    'num_embed': 5,\n",
    "    'num_trajectory': 3, \n",
    "    'match_score_thr': 0.5,\n",
    "    'max_age': 30,\n",
    "    'output':f'output/{vid_name}',\n",
    "    'out_txt':f'output/{vid_name}/info_xywh.txt',\n",
    "    'save_image': True,\n",
    "    'source': f'data/{vid_name}',\n",
    "    'trajectory_thresh': 0.9, \n",
    "    'trajectory_path':'weights/trajectory/best_400class_10_3.pth', \n",
    "    'reid_path':'weights/reid/reid_vehicle_new.pt',\n",
    "    'video_fps': 10, \n",
    "    'video_resolution': (1280, 720), \n",
    "#     'video_source': \"datasets/ai_challenge_2020/sample_02.mp4\",\n",
    "    'weights': 'weights/yolov5/last_19.pt', \n",
    "    }\n",
    "\n",
    "opt = dotdict(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qTvT99T4trCf",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634625854899,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "qTvT99T4trCf"
   },
   "outputs": [],
   "source": [
    "class DataCFG:\n",
    "    n_id = 400\n",
    "    window_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09dad5-2179-417b-8b5d-1c0d46a9a9ac",
   "metadata": {
    "id": "ef09dad5-2179-417b-8b5d-1c0d46a9a9ac"
   },
   "source": [
    "# Cosine + trajectory utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ppxARkUU08Wq",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634625854899,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "ppxARkUU08Wq"
   },
   "outputs": [],
   "source": [
    "def pad_sequence_fixed_size(sequences, batch_first=False, padding_value=0.0, max_len=256):\n",
    "  # based on torch.nn.utils.rnn.pad_sequence\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    \n",
    "    if batch_first:\n",
    "        out_dims = (len(sequences), max_len) + trailing_dims\n",
    "    else:\n",
    "        out_dims = (max_len, len(sequences)) + trailing_dims\n",
    "\n",
    "    out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = tensor.size(0)\n",
    "        # use index notation to prevent duplicate references to the tensor\n",
    "        if batch_first:\n",
    "            out_tensor[i, :length, ...] = tensor\n",
    "        else:\n",
    "            out_tensor[:length, i, ...] = tensor\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "def track2tensor(track, device, half):\n",
    "    if half:\n",
    "        item = torch.tensor(track, dtype=torch.float16, device=device).unsqueeze(0) \n",
    "    else:\n",
    "        item = torch.tensor(track, dtype=torch.float, device=device).unsqueeze(0) \n",
    "    item = pad_sequence_fixed_size(item, batch_first=True, max_len=DataCFG.window_size)\n",
    "    return item\n",
    "\n",
    "def get_normalized_box(box_info, img_shape):\n",
    "    h, w, c = img_shape\n",
    "    x1 = float(box_info[0])/w\n",
    "    x2 = float(box_info[1])/w\n",
    "    y1 = float(box_info[2])/h\n",
    "    y2 = float(box_info[3])/h\n",
    "    return [x1, x2, y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "CfKLBbl2rltc",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634625854900,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "CfKLBbl2rltc"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, units=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.fc1 = nn.Linear(input_size, input_size, bias=False)\n",
    "        self.fc2 = nn.Linear(input_size*2, self.units, bias = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        score_first_part = self.fc1(x)\n",
    "        h_t = x[:, -1, :] # Last hidden state\n",
    "        score = torch.einsum('ik,ijk->ij', h_t, score_first_part)\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "        context_vector = torch.einsum('ijk,ij->ik', x, attention_weights)\n",
    "        pre_activation = torch.cat([context_vector, h_t], dim=1)\n",
    "        attention_vector = torch.tanh(self.fc2(pre_activation))\n",
    "        return attention_vector\n",
    "\n",
    "class Trajectory(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=100, dropout=0.2, num_layers=3, batch_first=True)\n",
    "        self.attention = Attention(input_size=100, units=128)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(128, DataCFG.n_id)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop(out)\n",
    "        att = self.attention(out)\n",
    "        out = self.fc(att)\n",
    "        return out, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "MyGwOVN9bBoC",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634625854901,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "MyGwOVN9bBoC"
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_transform(half, device, track_boxes, det_box = None):\n",
    "    track = track_boxes[:]\n",
    "    if det_box:\n",
    "        track.append(det_box)\n",
    "    tensor = track2tensor(track, device, half)\n",
    "    return tensor\n",
    "\n",
    "@dask.delayed\n",
    "def predict(batch, model):\n",
    "    with torch.no_grad():\n",
    "        out, att = model(batch)\n",
    "    return att\n",
    "\n",
    "def get_trajectory_embeddings(tensors, dmodel):\n",
    "    batches = [dask.delayed(torch.cat)(batch, dim=0)\n",
    "            for batch in toolz.partition_all(opt.batch_size, tensors)]\n",
    "    predictions = [predict(batch, dmodel) for batch in batches]\n",
    "    predictions = dask.compute(*predictions)    \n",
    "    if len(predictions)>0:\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "    return predictions\n",
    "\n",
    "def get_trajectory_matrix(tracks_box_list, det_boxes_list, trajectory_model, euclidean_matrix, trajectory_thresh, device, half):\n",
    "    trajectory_matrix = {}\n",
    "    # Get track trajectory embedding\n",
    "    track_ids = sorted(tracks_box_list.keys())\n",
    "    tensors = [load_transform(half, device, tracks_box_list[k]) for k in track_ids]\n",
    "    track_predictions = get_trajectory_embeddings(tensors, trajectory_model)\n",
    "    \n",
    "    # Get candidate trajectory embedding\n",
    "    tensors = []\n",
    "    for k in track_ids:\n",
    "        for j in range(len(det_boxes_list)):\n",
    "            if euclidean_matrix[k][j] and len(tracks_box_list[k])>=opt.num_trajectory:\n",
    "                tensors.append(load_transform(half, device, tracks_box_list[k], det_boxes_list[j]))\n",
    "    predictions = get_trajectory_embeddings(tensors, trajectory_model)\n",
    "\n",
    "    count = 0\n",
    "    for i, k in enumerate(track_ids):\n",
    "        score = []    \n",
    "        for j in range(len(det_boxes_list)):\n",
    "            if euclidean_matrix[k][j]:\n",
    "                if len(tracks_box_list[k])<opt.num_trajectory:\n",
    "                    score.append(True)\n",
    "                else:\n",
    "                    track_emb = track_predictions[i]\n",
    "                    candidate_emb = predictions[count]\n",
    "                    count += 1\n",
    "                    score.append(float(F.cosine_similarity(track_emb, candidate_emb, dim=0)) >= trajectory_thresh)\n",
    "            else:\n",
    "                score.append(False)\n",
    "        trajectory_matrix[k] = score\n",
    "    return trajectory_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b809b327-90a7-4de2-a1a9-40abd1d3371e",
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1634625855218,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "b809b327-90a7-4de2-a1a9-40abd1d3371e"
   },
   "outputs": [],
   "source": [
    "def xyxy2cxcy(box):\n",
    "    cx = statistics.mean([box[1], box[0]])\n",
    "    cy = statistics.mean([box[3], box[2]])\n",
    "    return (cx, cy)\n",
    "\n",
    "def is_min_topk(a, k=1):\n",
    "    '''\n",
    "        Mask min top k values in array as 1, others as 0\n",
    "    '''\n",
    "    _, rix = np.unique(a, return_inverse=True)\n",
    "    return np.where(rix < k, True, False).reshape(a.shape)\n",
    "\n",
    "def get_euclidean_matrix(tracks_box_list, det_boxes_list, euclidean_num):\n",
    "    matrix = {}\n",
    "    track_ids = sorted(tracks_box_list.keys())\n",
    "    det_boxes_cxcy = [xyxy2cxcy(det_box) for det_box in det_boxes_list]\n",
    "    track_boxes_cxcy = []\n",
    "    for k in track_ids:\n",
    "        track_boxes_cxcy.append(xyxy2cxcy(tracks_box_list[k][-1]))\n",
    "    \n",
    "    scores = distance_matrix(track_boxes_cxcy, det_boxes_cxcy)\n",
    "    for i, k in enumerate(track_ids):\n",
    "        matrix[k] = is_min_topk(scores[i], euclidean_num)\n",
    "    return matrix\n",
    "\n",
    "def cosine_distance(X, Y, track_class_list, class_id, row2id, trajectory_matrix, INFY_COST=1e5):\n",
    "    \"\"\"Get cost matrix using cosine distance\"\"\"\n",
    "    norm_1 = torch.norm(X, dim=1, keepdim=True)\n",
    "    norm_2 = torch.norm(Y, dim=1, keepdim=True)\n",
    "    \n",
    "    cos_dis = 1 - (X@Y.T)/(norm_1@norm_2.T)\n",
    "    cos_dis = cos_dis.cpu().numpy()\n",
    "    for j in range(len(Y)):\n",
    "        for i in range(len(X)):\n",
    "            if track_class_list[i] != class_id[j] or not trajectory_matrix[row2id[i]][j]:\n",
    "                cos_dis[i][j] = INFY_COST\n",
    "    return cos_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Hc7Z3R9YV7a",
   "metadata": {
    "id": "6Hc7Z3R9YV7a"
   },
   "source": [
    "# Online Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "xwbb9Ux5cC0R",
   "metadata": {
    "executionInfo": {
     "elapsed": 1455,
     "status": "ok",
     "timestamp": 1634626358141,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "xwbb9Ux5cC0R"
   },
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        set_logging()\n",
    "        self.device = select_device(self.opt.device)\n",
    "        self.half = self.device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "        self.detector = self.load_detector(self.opt.weights)\n",
    "        self.reid = VehicleEmbedder(self.opt.reid_path)\n",
    "        self.trajectory_model = self.load_trajectory_model(self.opt.trajectory_path)\n",
    "\n",
    "        self.frame_id = 1\n",
    "        self.active_tracks = defaultdict(list)\n",
    "        self.age = OrderedDict()\n",
    "        self.num_tracks = 1\n",
    "\n",
    "        if not os.path.exists(self.opt.output):\n",
    "            os.mkdir(self.opt.output)\n",
    "        self.vid_writer = cv2.VideoWriter('/content/video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), self.opt.video_fps, self.opt.video_resolution)\n",
    "        self.check = defaultdict(list)\n",
    "        self.vehicle_count = [0,0,0,0]\n",
    "\n",
    "        self.f = open(opt.out_txt, 'w')\n",
    "    \n",
    "    def copy_video_to_destination(self):\n",
    "        shutil.copy('/content/video.mp4', os.path.join(self.opt.output, 'video.mp4'))\n",
    "\n",
    "    def load_detector(self, checkpoint_path):\n",
    "        model = attempt_load(checkpoint_path, map_location=self.device)  # load FP32 model\n",
    "        self.stride = int(model.stride.max())  # model stride\n",
    "        self.imgsz = check_img_size(self.opt.img_size, s=self.stride)  # check img_size\n",
    "        if self.half:\n",
    "            model.half()  # to FP16 \n",
    "        return model\n",
    "\n",
    "    def load_trajectory_model(self, trajectory_path):\n",
    "        model = Trajectory().to(self.device)\n",
    "        checkpoint = torch.load(trajectory_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        model.eval()\n",
    "        if self.half:\n",
    "            model.half()  # to FP16 \n",
    "        model = dask.delayed(model)\n",
    "        return model\n",
    "\n",
    "    def add_track_info(self, embed, cls, det_boxes, track=None):\n",
    "        if track is None:\n",
    "            track = defaultdict(list)\n",
    "        track['embed'].append(embed)\n",
    "        track['class'].append(cls)\n",
    "        track['norm_boxes'].append(det_boxes)\n",
    "\n",
    "        track['embed'] = track['embed'][-self.opt.num_embed:]\n",
    "        track['norm_boxes'] = track['norm_boxes'][-self.opt.num_trajectory:]\n",
    "        return track\n",
    "\n",
    "    def detect(self, img):\n",
    "        with torch.no_grad():\n",
    "            boxes = detectFromImage(self.detector, self.half, self.opt, self.device, self.imgsz, self.stride, img)\n",
    "            box_info = []\n",
    "            for b in boxes:\n",
    "                x1, y1, x2, y2 = map(int, [b.x_min, b.y_min, b.x_max, b.y_max])\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                cfs = 1\n",
    "                cls = b.semantic_id\n",
    "                box_info.append([self.frame_id, x1, y1, w, h, cfs, cls])\n",
    "        return box_info\n",
    "    \n",
    "    def get_embedding(self, img, box_info):\n",
    "        with torch.no_grad():            \n",
    "            crop_imgs = []\n",
    "            boxes = []\n",
    "            for info in box_info:\n",
    "                x1 = info[1]\n",
    "                y1 = info[2]\n",
    "                x2 = x1 + info[3]\n",
    "                y2 = y1 + info[4]\n",
    "                cfs = info[5]\n",
    "                cls = info[6]\n",
    "                if x1 < 0:\n",
    "                    x1 = 0\n",
    "                if x2 > img.shape[1]:\n",
    "                    x2 = img.shape[1]\n",
    "                if y1 < 0:\n",
    "                    y1 = 0\n",
    "                if y2 > img.shape[0]:\n",
    "                    y2 = img.shape[0]\n",
    "                \n",
    "                if x1 >= x2 or y1 >= y2:\n",
    "                    continue\n",
    "                else:\n",
    "                    cfs = info[5]\n",
    "                    cls = info[6]\n",
    "                    crop_imgs.append(img[y1:y2, x1:x2])\n",
    "                    boxes.append([x1, x2, y1, y2, cfs, cls])\n",
    "            if len(boxes)==0:\n",
    "                return OrderedDict()\n",
    "            \n",
    "            embed_list = self.reid.infer(crop_imgs)\n",
    "            embed_list = embed_list.cpu()\n",
    "            # Normalize embed\n",
    "            embed_dict = OrderedDict()\n",
    "            embed_dict['embed'] = F.normalize(embed_list)\n",
    "            embed_dict['boxes'] = boxes    \n",
    "            embed_dict['norm_boxes'] = [get_normalized_box(item, img.shape) for item in boxes]\n",
    "        return embed_dict\n",
    "    \n",
    "    def matching_cascade(self, id_list, embed_list, class_list, check, det_boxes_list):\n",
    "        row2id = {}\n",
    "        track_embed_list = []\n",
    "        track_class_list = []\n",
    "        tracks_box_list = {}\n",
    "        row_idx = 0\n",
    "\n",
    "        for id, info in self.active_tracks.items():\n",
    "            embeds = info['embed']\n",
    "            classes = info['class']\n",
    "            norm_boxes = info['norm_boxes']\n",
    "            tracks_box_list[id] = norm_boxes[-self.opt.num_trajectory:]\n",
    "            for i in range(len(embeds)):\n",
    "                track_embed_list.append(embeds[i])\n",
    "                track_class_list.append(classes[i])\n",
    "                row2id[row_idx] = id\n",
    "                row_idx += 1\n",
    "\n",
    "        euclidean_matrix = get_euclidean_matrix(tracks_box_list, det_boxes_list, self.opt.euclidean_num)\n",
    "        trajectory_matrix = get_trajectory_matrix(tracks_box_list, det_boxes_list, self.trajectory_model, euclidean_matrix, self.opt.trajectory_thresh, self.device, self.half)\n",
    "        reid_dists = cosine_distance(torch.stack(track_embed_list), embed_list, track_class_list, class_list, row2id, trajectory_matrix)\n",
    "        row, col = linear_sum_assignment(reid_dists)\n",
    "        un_matched = [] # Những embedding mới chưa được match với id nào\n",
    "        results = []\n",
    "        for r, c in zip(row, col):\n",
    "            dist = reid_dists[r, c]\n",
    "            results.append([r, c, dist, row2id[r]])\n",
    "        \n",
    "        results.sort(key=lambda x: x[2])\n",
    "        for r, c, dist, id in results:\n",
    "            if dist < self.opt.match_score_thr:\n",
    "                if not check[id]:\n",
    "                    # Nếu id chưa được match \n",
    "                    id_list[c] = id\n",
    "                    check[id] = True\n",
    "                    self.active_tracks[id] = self.add_track_info(embed_list[c], class_list[c], det_boxes_list[c], self.active_tracks[id])\n",
    "                    if len(self.active_tracks[id]['embed']) > self.opt.num_embed:\n",
    "                        self.active_tracks[id]['embed'] = self.active_tracks[id]['embed'][-self.opt.num_embed:]\n",
    "                        self.active_tracks[id]['norm_boxes'] = self.active_tracks[id]['norm_boxes'][-self.opt.num_trajectory:]\n",
    "                else:\n",
    "                    # Nếu id đã được matched rồi thì cho embedding đó vào tập chưa được match\n",
    "                    un_matched.append(c)\n",
    "            else:\n",
    "                id_list[c] = self.num_tracks\n",
    "                check[self.num_tracks] = True\n",
    "                self.active_tracks[self.num_tracks] = self.add_track_info(embed_list[c], class_list[c], det_boxes_list[c])\n",
    "                self.age[self.num_tracks] = 0\n",
    "                self.num_tracks += 1\n",
    "        \n",
    "        return un_matched, id_list, check\n",
    "\n",
    "    def matching(self, info):\n",
    "        id_dict = OrderedDict()\n",
    "\n",
    "        if len(info.keys())==0:\n",
    "            for id, v in self.age.items():\n",
    "                self.age[id] = self.age[id] + 1\n",
    "            del_id = []\n",
    "            for id, a in self.age.items():\n",
    "                if a > self.opt.max_age:\n",
    "                    del_id.append(id)\n",
    "            for id in del_id:\n",
    "                self.active_tracks.pop(id)\n",
    "                self.age.pop(id)\n",
    "            return id_dict\n",
    "        \n",
    "        embed_list = info['embed']\n",
    "        boxes = info['boxes']\n",
    "        norm_boxes = info['norm_boxes']\n",
    "        class_list = []\n",
    "        for i in range(len(boxes)):\n",
    "            class_list.append(boxes[i][5])\n",
    "        \n",
    "        if len(self.active_tracks.keys()) == 0: # No active tracks\n",
    "            id_list = list(range(self.num_tracks, self.num_tracks + embed_list.size(0)))\n",
    "            for i in range(embed_list.size(0)):\n",
    "                self.active_tracks[self.num_tracks + i] = self.add_track_info(embed_list[i-1], boxes[i-1][5], norm_boxes[i-1])\n",
    "                self.age[self.num_tracks + i] = 0\n",
    "            self.num_tracks = embed_list.size(0) + self.num_tracks\n",
    "        else:\n",
    "            id_list = [-1 for _ in range(embed_list.size(0))]\n",
    "            check = defaultdict(list) # Kiểm tra xem các id đã được match chưa, ban đầu giá trị bằng False\n",
    "            for id, info in self.active_tracks.items():\n",
    "                check[id] = False\n",
    "            \n",
    "            un_matched, id_list, check = self.matching_cascade(id_list, embed_list, class_list, check, norm_boxes)\n",
    "            # Nếu vẫn còn embedding mới chưa được match với id nào\n",
    "            if len(un_matched) != 0:\n",
    "                for c in un_matched:\n",
    "                    id_list[c] = self.num_tracks\n",
    "                    check[self.num_tracks] = True\n",
    "                    self.active_tracks[self.num_tracks] = self.add_track_info(embed_list[c], class_list[c], norm_boxes[c])\n",
    "                    self.age[self.num_tracks] = 0\n",
    "                    self.num_tracks += 1\n",
    "            for k, id in enumerate(id_list):\n",
    "                if id == -1:\n",
    "                    id_list[k] = self.num_tracks\n",
    "                    check[self.num_tracks] = True\n",
    "                    self.active_tracks[self.num_tracks] = self.add_track_info(embed_list[k], class_list[k], norm_boxes[k])\n",
    "                    self.age[self.num_tracks] = 0\n",
    "                    self.num_tracks += 1\n",
    "\n",
    "            for id, v in check.items():\n",
    "                if not v:\n",
    "                    self.age[id] = self.age[id] + 1\n",
    "            del_id = []\n",
    "            for id, a in self.age.items():\n",
    "                if a > self.opt.max_age:\n",
    "                    del_id.append(id)\n",
    "            for id in del_id:\n",
    "                self.active_tracks.pop(id)\n",
    "                self.age.pop(id)\n",
    "            \n",
    "        id_dict = {}\n",
    "        id_dict['id'] = id_list\n",
    "        id_dict['boxes'] = boxes\n",
    "        for i, box in enumerate(boxes):\n",
    "            if box[5]<4 and id_list[i] not in self.check:\n",
    "                self.check[id_list[i]] = True\n",
    "                self.vehicle_count[box[5]] += 1\n",
    "        return id_dict\n",
    "    \n",
    "    def process_one_frame(self, img, box_info=None):\n",
    "        if box_info is None:\n",
    "            box_info = self.detect(img)\n",
    "        embed_dict = self.get_embedding(img, box_info)\n",
    "        id_dict = self.matching(embed_dict)\n",
    "        self.frame_id += 1\n",
    "        return id_dict\n",
    "    \n",
    "    def save_result(self, image, info):\n",
    "        img = copy.deepcopy(image)\n",
    "        colors = [(0, 255, 255), (0, 0, 255)]\n",
    "        if len(info.keys())!=0:\n",
    "            boxes = info['boxes']\n",
    "            ids = info['id']\n",
    "            for i, box in enumerate(boxes):\n",
    "                cv2.rectangle(img, (box[0], box[2]), (box[1], box[3]), colors[1], 2)\n",
    "                cv2.putText(img, str(ids[i]), (box[0], box[2]), cv2.FONT_HERSHEY_SIMPLEX, 1, colors[0], 2)\n",
    "        cv2.rectangle(img, (0, 0), (int(img.shape[1]*0.3), int(img.shape[0]*0.07)), (0,0,0), -1)\n",
    "        cv2.putText(img, \"Frame: {} Motorcycle: {} Car: {} Pedestrian: {} Truck: {} Total: {}\".format(self.frame_id, *self.vehicle_count, sum(self.vehicle_count)), (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[0], 1)\n",
    "        if self.opt.save_image:\n",
    "            name_save = os.path.join(self.opt.output, str(self.frame_id-1).zfill(6) + '.jpg')\n",
    "            cv2.imwrite(name_save, img)\n",
    "        self.vid_writer.write(img)\n",
    "    \n",
    "    def save_txt(self, info):    \n",
    "        if len(info.keys()) != 0:\n",
    "            ids = info['id']\n",
    "            boxes = info['boxes']\n",
    "            results = []\n",
    "            for i, id in enumerate(ids):\n",
    "                x = boxes[i][0]\n",
    "                y = boxes[i][2]\n",
    "                w = boxes[i][1] - boxes[i][0]\n",
    "                h = boxes[i][3] - boxes[i][2]\n",
    "                cfs = boxes[i][4]\n",
    "                cls = boxes[i][5]\n",
    "                vs = 1\n",
    "                results.append([self.frame_id, id, x, y, w, h, cfs, cls, vs])\n",
    "\n",
    "            for i in range(len(results)):\n",
    "                self.f.write(str(results[i][0]) + ',' +  str(results[i][1]) + ',' + str(results[i][2]) + ',' + \\\n",
    "                        str(results[i][3]) + ',' + str(results[i][4]) + ',' + str(results[i][5]) + ',' + \\\n",
    "                        str(results[i][6]) + ',' + str(results[i][7]) + ',' + str(results[i][8]) + '\\n')\n",
    "\n",
    "    def save_txt_mot(self, info):\n",
    "        if len(info.keys()) != 0:\n",
    "            ids = info['id']\n",
    "            boxes = info['boxes']\n",
    "            results = []\n",
    "            for i, id in enumerate(ids):\n",
    "                x = boxes[i][0]\n",
    "                y = boxes[i][2]\n",
    "                w = boxes[i][1] - boxes[i][0]\n",
    "                h = boxes[i][3] - boxes[i][2]\n",
    "                cfs = boxes[i][4]\n",
    "                results.append([self.frame_id, id, x, y, w, h, cfs, -1, -1, -1])\n",
    "\n",
    "            for res in results:\n",
    "                self.f.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(*res))\n",
    "    \n",
    "    def release_all(self):\n",
    "        self.f.close()\n",
    "        self.vid_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iezJTIF2YT2Y",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634625856368,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "iezJTIF2YT2Y"
   },
   "outputs": [],
   "source": [
    "def load_images_path(source):\n",
    "    imgs = sorted(glob.glob(os.path.join(source, '*.jpg')))\n",
    "    # imgs = imgs[:min(opt.n_image, len(imgs))]\n",
    "    return imgs\n",
    "\n",
    "def get_box_from_file(source):\n",
    "    boxes = []\n",
    "    with open(source, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(\",\")\n",
    "        boxes.append([int(line[0]), int(line[2]), int(line[3]), int(line[4]), int(line[5]), int(line[6]), int(line[7]), float(line[8])])\n",
    "    return boxes\n",
    "\n",
    "def get_box_from_file_mot(source):\n",
    "    # [self.frame_id, x1, y1, w, h, cfs, cls]\n",
    "    boxes = []\n",
    "    with open(source, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(\",\") # frame, id, x1, y1, w, h, cfs, x, y, z\n",
    "        frame = int(line[0])\n",
    "        x1 = round(float(line[2]))\n",
    "        y1 = round(float(line[3]))\n",
    "        w = round(float(line[4]))\n",
    "        h = round(float(line[5]))\n",
    "        cfs = float(line[6])\n",
    "        cls = 1\n",
    "        boxes.append([frame, x1, y1, w, h, cfs, cls])\n",
    "    return boxes\n",
    "\n",
    "def get_frame_boxes(boxes, frame_id):\n",
    "    box_info = []\n",
    "    for box in boxes:\n",
    "        if box[0]==frame_id and box[5]>=0.25:\n",
    "            box_info.append(box)\n",
    "    return box_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "BV7i76Dwrb17",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1634625856369,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "BV7i76Dwrb17"
   },
   "outputs": [],
   "source": [
    "def frame_iter(capture, description=''):\n",
    "    def _iterator():\n",
    "        while capture.grab():\n",
    "            yield capture.retrieve()[1]\n",
    "\n",
    "    return tqdm(\n",
    "        _iterator(),\n",
    "        desc=description,\n",
    "        total=int(capture.get(CAP_PROP_FRAME_COUNT)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "JxoSx-CVs0dE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324075,
     "status": "ok",
     "timestamp": 1634626683943,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "JxoSx-CVs0dE",
    "outputId": "9545a5f8-a606-43be-9ac4-05fef1bd1af7"
   },
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture(opt.video_source)\n",
    "# input_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "# print(f\"Input fps {input_fps}\")\n",
    "# tracker = Tracker(opt)\n",
    "\n",
    "# for i, img in enumerate(frame_iter(video)):\n",
    "#     id_dict = tracker.process_one_frame(img)\n",
    "#     tracker.save_result(img, id_dict)\n",
    "#     tracker.save_txt(id_dict)\n",
    "\n",
    "# tracker.release_all()\n",
    "# video.release()\n",
    "# tracker.copy_video_to_destination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4OxBouiLzJA",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1634625980498,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "d4OxBouiLzJA"
   },
   "outputs": [],
   "source": [
    "imgs = load_images_path(opt.source)\n",
    "# boxes = get_box_from_file(opt.file_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799H-NIvbFB2",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1634625980499,
     "user": {
      "displayName": "Nhat Minh Phan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09725510368985845488"
     },
     "user_tz": -420
    },
    "id": "799H-NIvbFB2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2021-10-12 torch 1.10.1+cu102 CUDA:cuda (NVIDIA GeForce GTX 1060, 3071.8125MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87198694 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelecSLS42_B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 149/149 [00:56<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "tracker = Tracker(opt)\n",
    "\n",
    "for img_path in tqdm(imgs):\n",
    "    img = cv2.imread(img_path)\n",
    "    # box_info = get_frame_boxes(boxes, int(img_path.split('/')[-1].split('.')[0]))\n",
    "    id_dict = tracker.process_one_frame(img)  \n",
    "    tracker.save_result(img, id_dict)\n",
    "    tracker.save_txt(id_dict)\n",
    "# tracker.close_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb6d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "InferenceOnline_new.ipynb",
   "provenance": [
    {
     "file_id": "1X7namlaOCDwon0py-ZcwRsaMtLk99yBt",
     "timestamp": 1630406369266
    }
   ]
  },
  "kernelspec": {
   "display_name": "tracking",
   "language": "python",
   "name": "tracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
