{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "egT5B0nu8BUY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import json\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kei6Hoj_IEtb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt1yuEangyTU"
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minh PC\\tracking\\src\n"
     ]
    }
   ],
   "source": [
    "# Directory/to/your/repo\n",
    "%cd C:/Users/Minh PC/tracking/src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fyqf7UGvgfgJ"
   },
   "outputs": [],
   "source": [
    "data_source = \"data/trajectory_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL80C__tgqmM"
   },
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KDCLMjoahq_a"
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q-ahx97Pgoll"
   },
   "outputs": [],
   "source": [
    "class DataCFG:\n",
    "    n_id = 400\n",
    "    window_size = 10\n",
    "    window_sep = 1\n",
    "    min_seq_size = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZYqBbmigkGa6"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (1920, 2560, 3) # H, W, C\n",
    "\n",
    "def get_normalize_info(file_path):\n",
    "    # x: width \n",
    "    # y: height\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, names=[\"frame\",\"id\",\"vehicle_id\",\"x1\",\"x2\",\"y1\",\"y2\"])\n",
    "    df[\"x1_norm\"] = df.apply(lambda row: row['x1']/IMG_SHAPE[1], axis=1)\n",
    "    df[\"x2_norm\"] = df.apply(lambda row: row['x2']/IMG_SHAPE[1], axis=1)\n",
    "    df[\"y1_norm\"] = df.apply(lambda row: row['y1']/IMG_SHAPE[0], axis=1)\n",
    "    df[\"y2_norm\"] = df.apply(lambda row: row['y2']/IMG_SHAPE[0], axis=1)\n",
    "\n",
    "    # del df['x1']\n",
    "    # del df['x2']\n",
    "    # del df['y1']\n",
    "    # del df['y2']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_tracks(df, f):\n",
    "    tracks = []\n",
    "    vehicles = df['id'].unique()\n",
    "    for id in vehicles:\n",
    "        track = []\n",
    "        vehicle = df[df['id']==id].sort_values(by=['frame'])\n",
    "        for idx, row in vehicle.iterrows():\n",
    "            if not math.isnan(row['x1_norm']) and not math.isnan(row['x2_norm']) and not math.isnan(row['y1_norm']) and not math.isnan(row['y2_norm']):\n",
    "                track.append((row['x1_norm'], row['x2_norm'], row['y1_norm'], row['y2_norm']))\n",
    "            else:\n",
    "                print(f)\n",
    "                print((row['x1_norm'], row['x2_norm'], row['y1_norm'], row['y2_norm']))\n",
    "                print((row['x1'], row['x2'], row['y1'], row['y2']))\n",
    "        tracks.append(track)\n",
    "    return tracks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p1wc63I3j4Fv"
   },
   "outputs": [],
   "source": [
    "all_tracks = []\n",
    "for f in sorted(os.listdir(data_source)):\n",
    "    df = get_normalize_info(os.path.join(data_source, f))\n",
    "    all_tracks.extend(get_tracks(df, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPL6MJBBm4D8",
    "outputId": "ee3141c1-383c-41b7-9239-a0116d769378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y-ck0w8K9H-j"
   },
   "outputs": [],
   "source": [
    "def generate_trajectories(frames):\n",
    "    trajectories = []\n",
    "    for end in range(DataCFG.min_seq_size, len(frames) + DataCFG.window_size - DataCFG.min_seq_size + 1, DataCFG.window_sep):\n",
    "        trajectories.append(frames[max(0,end-DataCFG.window_size):min(len(frames), end)])\n",
    "    return trajectories\n",
    "\n",
    "# generate_trajectories(os.path.join(data_source, \"vungtau_107.0.avi_save-48.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbB1-2--hWSX",
    "outputId": "f2207646-b42f-4b59-90ae-4991011e25b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23795\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "ids = random.choices(range(len(all_tracks)), k=DataCFG.n_id)\n",
    "for i, filename in enumerate(ids):\n",
    "    trajectories = generate_trajectories(all_tracks[i])\n",
    "    for trajectory in trajectories:\n",
    "        dataset.append((trajectory, i))\n",
    "print(len(dataset))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2KjtwJhdDhF-",
    "outputId": "7f3d738b-d921-41cb-f606-e7b13fc967b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frames</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0.395, 0.4175, 0.06820937499999999, 0.133714...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(0.395, 0.4175, 0.06820937499999999, 0.133714...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(0.395, 0.4175, 0.06820937499999999, 0.133714...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0.395, 0.4175, 0.06820937499999999, 0.133714...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(0.395, 0.4175, 0.06820937499999999, 0.133714...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frames  id\n",
       "0  [(0.395, 0.4175, 0.06820937499999999, 0.133714...   0\n",
       "1  [(0.395, 0.4175, 0.06820937499999999, 0.133714...   0\n",
       "2  [(0.395, 0.4175, 0.06820937499999999, 0.133714...   0\n",
       "3  [(0.395, 0.4175, 0.06820937499999999, 0.133714...   0\n",
       "4  [(0.395, 0.4175, 0.06820937499999999, 0.133714...   0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset, columns = ['frames', 'id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLLQ1leAgwxT"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FlAY1-xtDyCK"
   },
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        row = row.to_dict()\n",
    "        # centers = []\n",
    "        # for frame in row['frames']:\n",
    "        #     centers.append((mean([frame[0],frame[1]]), mean([frame[0],frame[1]])))\n",
    "        item = {}\n",
    "        item['frames'] = torch.tensor(row['frames'], dtype=torch.float)\n",
    "        item['id'] = torch.tensor(row['id'])\n",
    "        return item\n",
    "\n",
    "def pad_sequence_fixed_size(sequences, batch_first=False, padding_value=0.0, max_len=256):\n",
    "  # based on torch.nn.utils.rnn.pad_sequence\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    \n",
    "    if batch_first:\n",
    "        out_dims = (len(sequences), max_len) + trailing_dims\n",
    "    else:\n",
    "        out_dims = (max_len, len(sequences)) + trailing_dims\n",
    "\n",
    "    out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = tensor.size(0)\n",
    "        # use index notation to prevent duplicate references to the tensor\n",
    "        if batch_first:\n",
    "            out_tensor[i, :length, ...] = tensor\n",
    "        else:\n",
    "            out_tensor[:length, i, ...] = tensor\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    frames = [item[\"frames\"] for item in batch]\n",
    "    frames_pad = pad_sequence_fixed_size(frames, batch_first=True, max_len=DataCFG.window_size) # N * seq_len * 4\n",
    "    labels = [item[\"id\"].unsqueeze(0) for item in batch] \n",
    "    labels = torch.cat(labels, dim=0) # N\n",
    "    return frames_pad, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN_CPtTUFOAM",
    "outputId": "cbf6a14c-6a4f-46b4-b75d-909a854a579f"
   },
   "outputs": [],
   "source": [
    "ds = TrajectoryDataset(df)\n",
    "dl = DataLoader(ds, batch_size=2,shuffle=True, collate_fn=Collate(), num_workers=2)\n",
    "for step, (frames, targets) in enumerate(dl):\n",
    "    print(frames)\n",
    "\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80C1r1tmgsMO"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LNBJXZApItLH"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, units=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.fc1 = nn.Linear(input_size, input_size, bias=False)\n",
    "        self.fc2 = nn.Linear(input_size*2, self.units, bias = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        score_first_part = self.fc1(x)\n",
    "        h_t = x[:, -1, :] # Last hidden state\n",
    "        score = torch.einsum('ik,ijk->ij', h_t, score_first_part)\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "        context_vector = torch.einsum('ijk,ij->ik', x, attention_weights)\n",
    "        pre_activation = torch.cat([context_vector, h_t], dim=1)\n",
    "        attention_vector = torch.tanh(self.fc2(pre_activation))\n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MULfTNaNT0lF"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=100, dropout=0.2, num_layers=3, batch_first=True)\n",
    "        self.attention = Attention(input_size=100, units=128)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(128, DataCFG.n_id)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop(out)\n",
    "        out = self.attention(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSk49-kpgtxv"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DXIJXVtobl74"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    n_fold = 5\n",
    "    lr = 1e-3\n",
    "    n_epochs = 100\n",
    "    steplr_step_size = 25\n",
    "    steplr_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PGaSZ9nVajOQ"
   },
   "outputs": [],
   "source": [
    "class LossMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.n += 1\n",
    "        # incremental update\n",
    "        self.avg = val / self.n + (self.n - 1) / self.n * self.avg\n",
    "\n",
    "        \n",
    "class AccMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "        \n",
    "    def update(self, y_true, y_pred):\n",
    "        y_pred = torch.argmax(y_pred,dim=1)\n",
    "        \n",
    "        y_true = y_true.cpu().numpy().astype(int)\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        last_n = self.n\n",
    "        self.n += len(y_true)\n",
    "        true_count = np.sum(y_true == y_pred)\n",
    "        # incremental update\n",
    "        self.avg = true_count / self.n + last_n / self.n * self.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UCxNaJrzawBG"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        loss_meter, \n",
    "        score_meter\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=CFG.steplr_step_size, gamma=CFG.steplr_gamma)\n",
    "        self.criterion = criterion\n",
    "        self.loss_meter = loss_meter\n",
    "        self.score_meter = score_meter\n",
    "        self.hist = {'val_loss':[],\n",
    "                     'val_score':[],\n",
    "                     'train_loss':[],\n",
    "                     'train_score':[]\n",
    "                    }\n",
    "        self.best_valid_score = -np.inf\n",
    "        self.best_valid_loss = np.inf\n",
    "        self.n_patience = 0\n",
    "        \n",
    "        self.messages = {\n",
    "            \"epoch\": \"[Epoch {}: {}] loss: {:.5f}, score: {:.5f}, time: {} s\",\n",
    "            \"checkpoint\": \"The score improved from {:.5f} to {:.5f}. Save model to '{}'\",\n",
    "            \"patience\": \"\\nValid score didn't improve last {} epochs.\"\n",
    "        }\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path, patience):\n",
    "        for n_epoch in range(1, epochs + 1):\n",
    "            self.info_message(\"EPOCH: {}\", n_epoch)\n",
    "            \n",
    "            train_loss, train_score, train_time = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_score, valid_time = self.valid_epoch(valid_loader)\n",
    "            \n",
    "            self.hist['val_loss'].append(valid_loss)\n",
    "            self.hist['train_loss'].append(train_loss)\n",
    "            self.hist['val_score'].append(valid_score)\n",
    "            self.hist['train_score'].append(train_score)\n",
    "            \n",
    "            self.info_message(\n",
    "                self.messages[\"epoch\"], \"Train\", n_epoch, train_loss, train_score, train_time\n",
    "            )\n",
    "            \n",
    "            self.info_message(\n",
    "                self.messages[\"epoch\"], \"Valid\", n_epoch, valid_loss, valid_score, valid_time\n",
    "            )\n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.info_message(\n",
    "                    self.messages[\"checkpoint\"], self.best_valid_score, valid_score, save_path\n",
    "                )\n",
    "                self.best_valid_score = valid_score\n",
    "                self.best_valid_loss = valid_loss\n",
    "                self.save_model(n_epoch, save_path)\n",
    "                self.n_patience = 0\n",
    "            else:\n",
    "                self.n_patience += 1\n",
    "            \n",
    "            if self.n_patience >= patience:\n",
    "                self.info_message(self.messages[\"patience\"], patience)\n",
    "                break\n",
    "            self.scheduler.step()\n",
    "                \n",
    "        return self.best_valid_loss, self.best_valid_score\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        t = time.time()\n",
    "        train_loss = self.loss_meter()\n",
    "        train_score = self.score_meter()\n",
    "        \n",
    "        for step, (frames, ids) in enumerate(train_loader, 1):\n",
    "            X = frames.to(self.device)\n",
    "            targets = ids.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X).squeeze(1)\n",
    "            \n",
    "            loss = self.criterion(outputs, targets)\n",
    "    \n",
    "            loss.backward()\n",
    "\n",
    "            train_loss.update(loss.detach().item())\n",
    "            train_score.update(targets, outputs.detach())\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            _loss, _score = train_loss.avg, train_score.avg\n",
    "            message = 'Train Step {}/{}, train_loss: {:.5f}, train_score: {:.5f}'\n",
    "            self.info_message(message, step, len(train_loader), _loss, _score, end=\"\\r\")\n",
    "        \n",
    "        return train_loss.avg, train_score.avg, int(time.time() - t)\n",
    "    \n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        t = time.time()\n",
    "        valid_loss = self.loss_meter()\n",
    "        valid_score = self.score_meter()\n",
    "\n",
    "        for step, (frames, ids) in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = frames.to(self.device)\n",
    "                targets = ids.to(self.device)\n",
    "\n",
    "                outputs = self.model(X).squeeze(1)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                valid_loss.update(loss.detach().item())\n",
    "                valid_score.update(targets, outputs)\n",
    "                \n",
    "            _loss, _score = valid_loss.avg, valid_score.avg\n",
    "            message = 'Valid Step {}/{}, valid_loss: {:.5f}, valid_score: {:.5f}'\n",
    "            self.info_message(message, step, len(valid_loader), _loss, _score, end=\"\\r\")\n",
    "        \n",
    "        return valid_loss.avg, valid_score.avg, int(time.time() - t)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Training Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        plt.plot(self.hist['train_loss'], label=\"Train\")\n",
    "        plt.plot(self.hist['val_loss'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_score(self):\n",
    "        plt.title(\"Score\")\n",
    "        plt.xlabel(\"Training Epochs\")\n",
    "        plt.ylabel(\"Acc\")\n",
    "\n",
    "        plt.plot(self.hist['train_score'], label=\"Train\")\n",
    "        plt.plot(self.hist['val_score'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, n_epoch, save_path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_message(message, *args, end=\"\\n\"):\n",
    "        print(message.format(*args), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AsJuN7Y4awuf",
    "outputId": "301eabae-45d8-44b4-8beb-756d23744e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_fold, shuffle = True, random_state = 2)\n",
    "t = df['id']\n",
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(np.zeros(len(t)), t), 1):\n",
    "    train_df = df.loc[train_index]\n",
    "    val_df = df.loc[val_index]\n",
    "    train_ds = TrajectoryDataset(train_df)\n",
    "    val_ds = TrajectoryDataset(val_df)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        collate_fn=Collate(),\n",
    "        num_workers=2,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate(),\n",
    "        num_workers=2,\n",
    "    )\n",
    "    \n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        LossMeter, \n",
    "        AccMeter\n",
    "    )\n",
    "    loss, score = trainer.fit(\n",
    "        CFG.n_epochs, \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        f\"output/best-model-{fold}.pth\", \n",
    "        100,\n",
    "    )\n",
    "    losses.append(loss)\n",
    "    scores.append(score)\n",
    "    \n",
    "    trainer.plot_loss()\n",
    "    trainer.plot_score()\n",
    "    break\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTraining complete in {:.0f}m {:.0f}s'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "print('Avg loss {}'.format(np.mean(losses)))\n",
    "print('Avg score {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqKKM-4-fjHr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\"losses\": losses, \"acc\": scores}\n",
    "with open('output/results.json', 'w') as file:\n",
    "     file.write(json.dumps(results)) # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPYka-uwSUTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tracking",
   "language": "python",
   "name": "tracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
